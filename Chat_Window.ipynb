{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 1)\n",
      "(48, 1)\n",
      "(98, 1)\n",
      "(94, 1)\n",
      "(198, 1)\n"
     ]
    }
   ],
   "source": [
    "Aegis_Domain= pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/ED_1.1.csv\")\n",
    "Data_Mining_General=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/2.csv\")\n",
    "Pythom_R_ML=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/3.csv\")\n",
    "Statistics=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/4.csv\")\n",
    "Hadoop=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/5.csv\")\n",
    "\n",
    "\n",
    "##\n",
    "#Size of each data set\n",
    "print(Aegis_Domain.shape)\n",
    "print(Data_Mining_General.shape)\n",
    "print(Pythom_R_ML.shape)\n",
    "print(Statistics.shape)\n",
    "print(Hadoop.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pre processing the data\n",
    "import re\n",
    "def cleaning_text(review):\n",
    "    cleaned =re.sub(\"[^a-zA-Z/:.?]\", \" \",review)\n",
    "    words =cleaned.lower().split()\n",
    "    return(\" \".join( words ))\n",
    "new_words=[]\n",
    "for i in range(0,Aegis_Domain.size ):\n",
    "    new_words.append(cleaning_text(Aegis_Domain[\"QA\"][i]))\n",
    "for i in range(0,Data_Mining_General.size ):\n",
    "    new_words.append(cleaning_text(Data_Mining_General[\"QA\"][i]))\n",
    "for i in range(0,Pythom_R_ML.size ):\n",
    "    new_words.append(cleaning_text(Pythom_R_ML[\"QA\"][i]))\n",
    "for i in range(0,Statistics.size ):\n",
    "    new_words.append(cleaning_text(Statistics[\"QA\"][i]))\n",
    "for i in range(0,Hadoop.size ):\n",
    "    new_words.append(cleaning_text(Hadoop[\"QA\"][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Extra data: line 1 column 3 - line 1 column 297118 (char 2 - 297117)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58a2306f46fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;31m#training the conversation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mchatbot_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[1;31m#Import corpus.english for basic greetings and casual replies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\chatterbot\\trainers.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconversation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mstatement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_or_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatement_history\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\chatterbot\\trainers.pyc\u001b[0m in \u001b[0;36mget_or_create\u001b[0;34m(self, statement_text)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstatement\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mdoes\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \"\"\"\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mstatement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\chatterbot\\storage\\jsonfile.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, statement_text)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstatement_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\jsondb\\db.pyc\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[1;31m# If only a key was provided return the corresponding value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[1;31m# if a key and a value are passed in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\jsondb\\db.pyc\u001b[0m in \u001b[0;36m_get_content\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\jsondb\\file_writer.pyc\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\jsondb\\compat.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mbson\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mjson_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parse_constant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\json\\decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Extra data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Extra data: line 1 column 3 - line 1 column 297118 (char 2 - 297117)"
     ]
    }
   ],
   "source": [
    "from nltk import corpus\n",
    "from chatterbot import ChatBot\n",
    "import logging\n",
    "\n",
    "\n",
    "# Uncomment the following line to enable verbose logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a new instance of a ChatBot\n",
    "chatbot_1 = ChatBot(\"Chatter Bot Instance\", \n",
    "    storage_adapter=\"chatterbot.storage.JsonFileStorageAdapter\",\n",
    "    logic_adapters=[\n",
    "        \"chatterbot.logic.MathematicalEvaluation\",\n",
    "        \"chatterbot.logic.TimeLogicAdapter\",\n",
    "        \"chatterbot.logic.BestMatch\"\n",
    "    ],\n",
    "    input_adapter=\"chatterbot.input.TerminalAdapter\",\n",
    "    output_adapter=\"chatterbot.output.TerminalAdapter\",\n",
    "    database=\"../database.db\"\n",
    ")\n",
    "\n",
    "\n",
    "#Import List trainer for training the chat history\n",
    "#\n",
    "from chatterbot.trainers import ListTrainer\n",
    "#\n",
    "#Move the preprocessed chat history to conversations\n",
    "#\n",
    "conversation=new_words\n",
    "chatbot_1.set_trainer(ListTrainer)\n",
    "#\n",
    "#training the conversation\n",
    "#\n",
    "chatbot_1.train(conversation)\n",
    "#\n",
    "#Import corpus.english for basic greetings and casual replies\n",
    "#\n",
    "#chatbot1.train(\"chatterbot.corpus.english\")\n",
    "#chatbot1.set_trainer(ChatterBotCorpusTrainer)\n",
    "#chatbot1.train(\"chatterbot.corpus.english\")\n",
    "#chatbot1.train(\n",
    "#    \"chatterbot.corpus.english.greetings\",\n",
    "#    \"chatterbot.corpus.english.conversations\"\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_____Sentiment Analysis ________#\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/chat_sentiment.csv\")\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "##\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "print df1.columns\n",
    "\n",
    "clean_review = review_to_words( df1[\"Questions\"][0] )\n",
    "print clean_review\n",
    "\n",
    "\n",
    "num_reviews = df1[\"Questions\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( df1[\"Questions\"][i] ) )\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    " \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print vocab\n",
    "\n",
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, df1[\"Sentiment\"] )\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "clean_test_reviews = [] \n",
    "clean_review = review_to_words(\"what is eligibility criteria\" )\n",
    "clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "#print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Hi, Welcome to mUni for PGP/EPGP in Data Science.\"\n",
    "print \"Business Analytics & Big Data in association with IBM.\"\n",
    "print \"please spend some time on this page to get all details and then apply online.\"\n",
    "print \"If u need any help let us know. Or call us at 8422970034, 8828084908, 9022137010,\"\n",
    "print(\"\\nType something to begin...\")\n",
    "\n",
    "# The following loop will execute each time the user enters input\n",
    "while True:\n",
    "    try:\n",
    "        # We pass None to this method because the parameter\n",
    "        # is not used by the TerminalAdapter\n",
    "        bot_input = chatbot1.get_response(None)\n",
    "        \n",
    "        clean_test_reviews = [] \n",
    "        clean_review = review_to_words(bot_input )\n",
    "        clean_test_reviews.append( clean_review )\n",
    "\n",
    "        # Get a bag of words for the test set, and convert to a numpy array\n",
    "        test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "        test_data_features = test_data_features.toarray()\n",
    "\n",
    "        # Use the random forest to make sentiment label predictions\n",
    "        result = forest.predict(test_data_features)\n",
    "        print result\n",
    "\n",
    "    # Press ctrl-c or ctrl-d on the keyboard to exit\n",
    "    except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
