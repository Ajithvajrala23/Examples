{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/ED_1.1.csv\")\n",
    "\n",
    "df.size\n",
    "#print df[\"QA\"][391]\n",
    "import re\n",
    "#def cleaning_text(review):\n",
    "    #cleaned =re.sub(\"[^a-zA-Z/:.?]\", \" \",review)\n",
    "    #words =cleaned.lower().split()\n",
    "    #return(\" \".join( words ))\n",
    "new_words =[]\n",
    "for i in range(0,df.size -1, 2):\n",
    "    new_words.append(df[\"QA\"][i])\n",
    "\n",
    "new_words\n",
    "df1 = pd.DataFrame({'Questions': new_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print df1\n",
    "#df1 = df1.drop_duplicates()\n",
    "df1.to_csv(\"chat_sentiment.csv\", index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/chat_sentiment.csv\")\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Questions', u'Sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business analytics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\Ajith Vajrala\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "clean_review = review_to_words( df1[\"Questions\"][0] )\n",
    "print clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_reviews = df1[\"Questions\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( df1[\"Questions\"][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    " \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'able', u'accepted', u'access', u'admission', u'aegis', u'analytics', u'application', u'apply', u'asked', u'attend', u'available', u'babd', u'better', u'bi', u'big', u'business', u'campus', u'career', u'center', u'certification', u'change', u'class', u'coding', u'companies', u'course', u'criteria', u'curriculum', u'data', u'dates', u'deal', u'delivery', u'depend', u'different', u'done', u'education', u'eligibile', u'eligibility', u'end', u'epgp', u'ex', u'executive', u'expected', u'experience', u'faculty', u'feedback', u'field', u'find', u'free', u'full', u'future', u'get', u'good', u'graduate', u'guarantee', u'guaranteed', u'guarnatee', u'hybrid', u'ibm', u'intelligence', u'intenship', u'internship', u'interview', u'involved', u'job', u'jobs', u'join', u'kind', u'know', u'loan', u'location', u'mba', u'means', u'members', u'middle', u'miss', u'missed', u'mode', u'model', u'models', u'module', u'ms', u'much', u'necessary', u'need', u'needed', u'old', u'online', u'opinion', u'opportunity', u'overall', u'paid', u'participants', u'pg', u'placement', u'possible', u'post', u'procedure', u'process', u'profile', u'program', u'project', u'prospect', u'provide', u'provided', u'provision', u'questions', u'recognized', u'regarding', u'required', u'rich', u'role', u'schedule', u'scholarship', u'science', u'scope', u'software', u'softwares', u'started', u'students', u'study', u'suggest', u'suit', u'suitable', u'switch', u'taken', u'talk', u'taught', u'teaching', u'time', u'timings', u'tools', u'touch', u'use', u'using', u'usp', u'various', u'warehouse', u'weekend', u'years']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, df1[\"Sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Interested']\n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = [] \n",
    "clean_review = review_to_words(\"what is eligibility criteria\" )\n",
    "clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
