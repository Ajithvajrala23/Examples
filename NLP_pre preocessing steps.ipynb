{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     NLP Preprocessing\n",
    "Since the Data set is very big,  we are unable to get the subset of the 900MB data set. We ran it for 3hrs still we are unable to get the subset of the data. \n",
    "\n",
    "So we have done preprocessing steps on 5MB data set provided. So Approach will be same, once we have finished complete analysis, we can extend the code to 50MB data set.\n",
    "\n",
    "Steps involved in Yelp data Set challenge:\n",
    "\n",
    "Approach1: \n",
    "\n",
    "    1.  Get the Data.\n",
    "    2.  Tokenizing, Parsing, Converting text to lower case, Stopword removal, Stemming.\n",
    "    3.  Convert the text in the form of tf-idf matrix\n",
    "    4.  Apply K-means clustering Algorithm\n",
    "    5.  Divide into 5 different clusters and identify the unique words belongs to each cluster.\n",
    "    6.  Caluclate the sentiment Analysis for each review\n",
    "    7.  Categorise the data grouped by \"Business-ID\" i.e. resturant-ID\n",
    "    8.  Take each review at a time and identify whether it belongs to First Cluster or not., repeated for all the reviews.\n",
    "    9.  Step-8 is repeated for all the clusters individally.\n",
    "    10. Now Using the result from sentiment Analysis and reviews-classification(if first review belongs to 1st and 3rd \n",
    "        cluster then the ouput will look like \"10100\")., we categorise the Sentiment Distribution.\n",
    "Approach2: \n",
    "\n",
    "    1.  Get the Data.\n",
    "    2.  Tokenizing, Parsing, Converting text to lower case, Stopword removal, Stemming.\n",
    "    3.  Convert the text in the form of tf-idf matrix\n",
    "    4.  Apply K-means clustering Algorithm\n",
    "    5.  Divide into 5 different clusters and identify the unique words belongs to each cluster.\n",
    "    6.  Categorise the data grouped by \"Business-ID\" i.e. resturant-ID\n",
    "    7.  Take each review and extract the sentence and identify the sentence belongs to which cluster and apply sentiment \n",
    "        analysis.\n",
    "    8.  Apply step-7 to all the reviews. Then we can get the Sentiment Distribution.\n",
    "    \n",
    " Team Members: Ajith vajrala, Basant Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\Aegis\\NLP\\Project/yelp_data_subset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtest = data\n",
    "#Converting list to array\n",
    "import numpy as np\n",
    "myarray = np.asarray(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting list to data frame using Pandas\n",
    "\n",
    "df = pd.DataFrame(dtest, columns=['votes', 'user_id', 'review_id', 'text','business_id','date','type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>votes</th>\n",
       "      <th>user_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{u'funny': 0, u'useful': 0, u'cool': 0}</td>\n",
       "      <td>PUFPaY9KxDAcGqfsorJp3Q</td>\n",
       "      <td>Ya85v4eqdd6k9Od8HbQjyA</td>\n",
       "      <td>Mr Hoagie is an institution. Walking in, it do...</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{u'funny': 0, u'useful': 0, u'cool': 0}</td>\n",
       "      <td>Iu6AxdBYGR4A0wspR9BYHA</td>\n",
       "      <td>KPvLNJ21_4wbYNctrOwWdQ</td>\n",
       "      <td>Excellent food. Superb customer service. I mis...</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>2014-02-13</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{u'funny': 1, u'useful': 1, u'cool': 0}</td>\n",
       "      <td>auESFwWvW42h6alXgFxAXQ</td>\n",
       "      <td>fFSoGV46Yxuwbr3fHNuZig</td>\n",
       "      <td>Yes this place is a little out dated and not o...</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>2015-10-31</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{u'funny': 0, u'useful': 0, u'cool': 0}</td>\n",
       "      <td>qiczib2fO_1VBG8IoCGvVg</td>\n",
       "      <td>pVMIt0a_QsKtuDfWVfSk2A</td>\n",
       "      <td>PROS: Italian hoagie was delicious.  Friendly ...</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{u'funny': 0, u'useful': 1, u'cool': 0}</td>\n",
       "      <td>qEE5EvV-f-s7yHC0Z4ydJQ</td>\n",
       "      <td>AEyiQ_Y44isJmNbMTyoMKQ</td>\n",
       "      <td>First the only reason this place could possibl...</td>\n",
       "      <td>5UmKMjUEUNdYWqANhGckJw</td>\n",
       "      <td>2016-04-08</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     votes                 user_id  \\\n",
       "0  {u'funny': 0, u'useful': 0, u'cool': 0}  PUFPaY9KxDAcGqfsorJp3Q   \n",
       "1  {u'funny': 0, u'useful': 0, u'cool': 0}  Iu6AxdBYGR4A0wspR9BYHA   \n",
       "2  {u'funny': 1, u'useful': 1, u'cool': 0}  auESFwWvW42h6alXgFxAXQ   \n",
       "3  {u'funny': 0, u'useful': 0, u'cool': 0}  qiczib2fO_1VBG8IoCGvVg   \n",
       "4  {u'funny': 0, u'useful': 1, u'cool': 0}  qEE5EvV-f-s7yHC0Z4ydJQ   \n",
       "\n",
       "                review_id                                               text  \\\n",
       "0  Ya85v4eqdd6k9Od8HbQjyA  Mr Hoagie is an institution. Walking in, it do...   \n",
       "1  KPvLNJ21_4wbYNctrOwWdQ  Excellent food. Superb customer service. I mis...   \n",
       "2  fFSoGV46Yxuwbr3fHNuZig  Yes this place is a little out dated and not o...   \n",
       "3  pVMIt0a_QsKtuDfWVfSk2A  PROS: Italian hoagie was delicious.  Friendly ...   \n",
       "4  AEyiQ_Y44isJmNbMTyoMKQ  First the only reason this place could possibl...   \n",
       "\n",
       "              business_id        date    type  \n",
       "0  5UmKMjUEUNdYWqANhGckJw  2012-08-01  review  \n",
       "1  5UmKMjUEUNdYWqANhGckJw  2014-02-13  review  \n",
       "2  5UmKMjUEUNdYWqANhGckJw  2015-10-31  review  \n",
       "3  5UmKMjUEUNdYWqANhGckJw  2015-12-26  review  \n",
       "4  5UmKMjUEUNdYWqANhGckJw  2016-04-08  review  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5503, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "votes          0\n",
       "user_id        0\n",
       "review_id      0\n",
       "text           0\n",
       "business_id    0\n",
       "date           0\n",
       "type           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data extraction is completed.\n",
    "#\n",
    "#Check for missing values in the data\n",
    "#\n",
    "df.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#There are no missing values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id        3519\n",
       "review_id      5503\n",
       "text           5499\n",
       "business_id     311\n",
       "date           2185\n",
       "type              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df\n",
    "del df_['votes']\n",
    "\n",
    "#Look at no.of categorical variables in the data\n",
    "df_.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "#\n",
    "#\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Reviews_text = df_['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "import re\n",
    "#\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in Reviews_text:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
