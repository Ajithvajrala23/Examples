{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import library Pandas to work on data frames\n",
    "import pandas as pd\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430, 1)\n",
      "(48, 1)\n",
      "(98, 1)\n",
      "(100, 1)\n",
      "(198, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/ED_1.1.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read())  # or readline if the file is large\n",
    "\n",
    "Aegis_Domain= pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/ED_1.1.csv\", encoding=result['encoding'])\n",
    "Data_Mining_General=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/2.csv\", encoding=result['encoding'])\n",
    "Pythom_R_ML=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/3.csv\", encoding=result['encoding'])\n",
    "Statistics=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/4.csv\", encoding=result['encoding'])\n",
    "Hadoop=pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/5.csv\", encoding=result['encoding'])\n",
    "\n",
    "\n",
    "##\n",
    "#Size of each data set\n",
    "print(Aegis_Domain.shape)\n",
    "print(Data_Mining_General.shape)\n",
    "print(Pythom_R_ML.shape)\n",
    "print(Statistics.shape)\n",
    "print(Hadoop.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pre processing the data\n",
    "import re\n",
    "def cleaning_text(review):\n",
    "    cleaned =re.sub(\"[^a-zA-Z/:.?]\", \" \",review)\n",
    "    words =cleaned.split()\n",
    "    return(\" \".join( words ))\n",
    "new_words=[]\n",
    "for i in range(0,Aegis_Domain.size):\n",
    "    new_words.append(cleaning_text(Aegis_Domain[\"QA\"][i]))\n",
    "for i in range(0,Data_Mining_General.size):\n",
    "    new_words.append(cleaning_text(Data_Mining_General[\"QA\"][i]))\n",
    "for i in range(0,Pythom_R_ML.size):\n",
    "    new_words.append(cleaning_text(Pythom_R_ML[\"QA\"][i]))\n",
    "for i in range(0,Statistics.size):\n",
    "    new_words.append(cleaning_text(Statistics[\"QA\"][i]))\n",
    "for i in range(0,Hadoop.size):\n",
    "    new_words.append(cleaning_text(Hadoop[\"QA\"][i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(874, 1)\n",
      "874\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(new_words, columns=['QA'])\n",
    "print(df.shape)\n",
    "print(df.size)\n",
    "#print df.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\site-packages\\chatterbot\\storage\\jsonfile.py:26: UnsuitableForProductionWarning: The JsonFileStorageAdapter is not recommended for production environments.\n",
      "  self.UnsuitableForProductionWarning\n"
     ]
    }
   ],
   "source": [
    "from nltk import corpus\n",
    "from chatterbot import ChatBot\n",
    "#import logging\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "# Uncomment the following line to enable verbose logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a new instance of a ChatBot\n",
    "chatbot2 = ChatBot(\"M_University_ChatBot\", \n",
    "    storage_adapter=\"chatterbot.storage.JsonFileStorageAdapter\",\n",
    "    logic_adapters=[\n",
    "        \"chatterbot.logic.MathematicalEvaluation\",\n",
    "        \"chatterbot.logic.TimeLogicAdapter\",\n",
    "        \"chatterbot.logic.BestMatch\"\n",
    "    ],\n",
    "    input_adapter=\"chatterbot.input.TerminalAdapter\",\n",
    "    output_adapter=\"chatterbot.output.TerminalAdapter\",\n",
    "    database=\"../database10.db\", read_only=True\n",
    ")\n",
    "\n",
    "\n",
    "#Import List trainer for training the chat history\n",
    "#\n",
    "from chatterbot.trainers import ListTrainer\n",
    "#\n",
    "#Move the preprocessed chat history to conversations\n",
    "#\n",
    "conversation=new_words\n",
    "chatbot2.set_trainer(ListTrainer)\n",
    "#\n",
    "#training the conversation\n",
    "#\n",
    "chatbot2.train(conversation)\n",
    "chatbot2.set_trainer(ChatterBotCorpusTrainer)\n",
    "\n",
    "chatbot2.train(\n",
    "    \"chatterbot.corpus.english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Questions', 'Sentiment'], dtype='object')\n",
      "business analytics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#_____Sentiment Analysis ________#\n",
    "with open(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/chat_sentiment.csv\", 'rb') as f:\n",
    "    result = chardet.detect(f.read()) \n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(r\"E:\\Aegis\\Project\\Data_set\\rechatterbotpdf/chat_sentiment.csv\", encoding=result['encoding'])\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "##\n",
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "print(df1.columns)\n",
    "\n",
    "clean_review = review_to_words( df1[\"Questions\"][0] )\n",
    "print(clean_review)\n",
    "\n",
    "\n",
    "num_reviews = df1[\"Questions\"].size\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in range( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_to_words( df1[\"Questions\"][i] ) )\n",
    "    \n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    " \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000) \n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "#print vocab\n",
    "\n",
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, df1[\"Sentiment\"] )\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "clean_test_reviews = [] \n",
    "clean_review = review_to_words(\"what is eligibility criteria\" )\n",
    "clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "#print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#____Text classification____#\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "#Word stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 sentences of training data\n"
     ]
    }
   ],
   "source": [
    "# Step2\n",
    "# provide 3 classes of training data\n",
    "training_data=[]\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how are you?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is your day?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"good day\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"how is it going today?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"hi Good morning?\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"nice thank you\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"its okay fine\"})\n",
    "training_data.append({\"class\":\"greeting\", \"sentence\":\"Welcome\"})\n",
    "##\n",
    "training_data.append({\"class\":\"Aegis\", \"sentence\":\"Where is Aegis located?\"})\n",
    "training_data.append({\"class\":\"Aegis\", \"sentence\":\"How Aegis related to IBM?\"})\n",
    "training_data.append({\"class\":\"Aegis\", \"sentence\":\"What is Aegis?\"})\n",
    "#\n",
    "#\n",
    "training_data.append({\"class\":\"Study Loan\", \"sentence\":\"Will I get study loan?\"})\n",
    "training_data.append({\"class\":\"Study Loan\", \"sentence\":\"Can I get Scholarship\"})\n",
    "training_data.append({\"class\":\"Study Loan\", \"sentence\":\"Which bank will provide study loan?\"})\n",
    "#\n",
    "#\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"Who will be teaching R?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"R lecturer?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"Who will be teaching Python?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"Python lecturer?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"faculty for R?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"faculty for python?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"faculty for Hadoop?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"Who will be teaching hadoop?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"Hadoop lecturer?\"})\n",
    "training_data.append({\"class\":\"Faculty\", \"sentence\":\"faculty for Statistics and probability?\"})\n",
    "#\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"Who will be teaching R?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"R lecturer?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"faculty for R?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"What is R programming?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"What is better for Data Analytics Python or R?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"Python or R?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"How course evaluation for R is done?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"R course curriculum?\"})\n",
    "training_data.append({\"class\":\"R Program\", \"sentence\":\"Should I have any prior experience for learning R or Python?\"})\n",
    "#\n",
    "#\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Who will be teaching Python?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Python lecturer?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Faculty for Python?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"What is use of Python in data Analytics?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Python course curriculum?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Python or R?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"Should I have any prior experience for learning R or Python?\"})\n",
    "training_data.append({\"class\":\"Python\", \"sentence\":\"How course evaluation for Python is done?\"})\n",
    "#\n",
    "#\n",
    "#\n",
    "print (\"%s sentences of training data\" % len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# capture unique stemmed words in the training corpus\n",
    "corpus_words = {}\n",
    "class_words = {}\n",
    "# turn a list into a set (of unique items) and then a list again (this removes duplicates)\n",
    "classes = list(set([a['class'] for a in training_data]))\n",
    "for c in classes:\n",
    "    # prepare a list of words within each class\n",
    "    class_words[c] = []\n",
    "\n",
    "# loop through each sentence in our training data\n",
    "for data in training_data:\n",
    "    # tokenize each sentence into words\n",
    "    for word in nltk.word_tokenize(data['sentence']):\n",
    "        # ignore a some things\n",
    "        if word not in [\"?\", \"'s\"]:\n",
    "            # stem and lowercase each word\n",
    "            stemmed_word = stemmer.stem(word.lower())\n",
    "            # have we not seen this word already?\n",
    "            if stemmed_word not in corpus_words:\n",
    "                corpus_words[stemmed_word] = 1\n",
    "            else:\n",
    "                corpus_words[stemmed_word] += 1\n",
    "\n",
    "            # add the word to our words in class list\n",
    "            class_words[data['class']].extend([stemmed_word])\n",
    "\n",
    "# we now have each stemmed word and the number of occurances of the word in our training corpus (the word's commonality)\n",
    "#print (\"Corpus words and counts: %s \\n\" % corpus_words)\n",
    "# also we have all words in each class\n",
    "#print (\"Class words: %s\" % class_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "esw = set(stopwords.words(\"english\"))\n",
    "\n",
    "# calculate a score for a given class\n",
    "def calculate_class_score(sentence, class_name, show_details=True):\n",
    "    score = 0\n",
    "    # tokenize each word in our new sentence\n",
    "    neat_words = [w for w in word_tokenize(sentence) if w not in esw]\n",
    "    for word in neat_words:\n",
    "        # check to see if the stem of the word is in any of our classes\n",
    "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
    "            # treat each word with same weight\n",
    "            score += 1\n",
    "            \n",
    "            if show_details:\n",
    "                print (\"   match: %s\" % stemmer.stem(word.lower() ))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    high_class = None\n",
    "    high_score = 0\n",
    "    # loop through our classes\n",
    "    for c in class_words.keys():\n",
    "        # calculate score of sentence for each class\n",
    "        score = calculate_class_score(sentence, c, show_details=False)\n",
    "        # keep track of highest score\n",
    "        if score > high_score:\n",
    "            high_class = c\n",
    "            high_score = score\n",
    "\n",
    "    return high_class, high_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('greeting', 1)\n",
      "(None, 0)\n",
      "('greeting', 2)\n",
      "(None, 0)\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"hi\"))\n",
    "print(classify(\"Hello\"))\n",
    "print(classify(\"Good Morning\"))\n",
    "print(classify(\"Gud Mrng\"))\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aegis', 2)\n",
      "('Aegis', 1)\n",
      "(None, 0)\n",
      "(None, 0)\n",
      "('R Program', 3)\n",
      "('Python', 2)\n",
      "('Aegis', 1)\n",
      "(None, 0)\n",
      "('Python', 2)\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"What is aegis?\"))\n",
    "print(classify(\"What is the class schedule\"))\n",
    "print(classify(\"is there any internship?\"))\n",
    "print(classify(\"Internsip??\"))\n",
    "print(classify(\"Which is better R or python?\"))\n",
    "print(classify(\"r or python??\"))\n",
    "print(classify(\"What is this capstone project?\"))\n",
    "print(classify(\"no idea what capstone is?\"))\n",
    "print(classify(\"What is the difference between online course?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Faculty', 1)\n",
      "('Aegis', 1)\n",
      "(None, 0)\n",
      "(None, 0)\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"what is hadoop?\"))\n",
    "print(classify(\"What is logistic Regression?\"))\n",
    "print(classify(\"HDFS ?\"))\n",
    "print(classify(\"how do we do indexing in HDFS??\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Python', 2)\n",
      "('Python', 2)\n",
      "('Python', 3)\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"What is machine learning?\"))\n",
    "print(classify(\"Teacher of machine learning?\"))\n",
    "print(classify(\"how course evaluation is done?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aegis', 1)\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"relation between data science and statistics?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type something to begin...\n",
      "Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Ajith Vajrala\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "['Interested']\n",
      "How are you babe\n",
      "I am fine. What about you.\n",
      "['Interested']\n",
      "how good is aegis\n",
      "Check about Aegis at www.aegis.edu.in and www.bellaward.com\n",
      "['Interested']\n",
      "should i join aegis or not\n",
      "Yes it is good to have the basic knowledge and understanding of Java as there is a tool Hadoop that requires the usage of java. However if you don t have any idea of java then Aegis and IBM s faculty will guide you.\n",
      "['Interested']\n"
     ]
    }
   ],
   "source": [
    "print(\"Type something to begin...\")\n",
    "\n",
    "# The following loop will execute each time the user enters input\n",
    "while True:\n",
    "    try:\n",
    "        # We pass None to this method because the parameter\n",
    "        # is not used by the TerminalAdapter\n",
    "        bot_input = chatbot2.get_response(None)\n",
    "        \n",
    "        clean_test_reviews = [] \n",
    "        clean_review = review_to_words(bot_input )\n",
    "        clean_test_reviews.append( clean_review )\n",
    "\n",
    "        # Get a bag of words for the test set, and convert to a numpy array\n",
    "        test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "        test_data_features = test_data_features.toarray()\n",
    "\n",
    "        # Use the random forest to make sentiment label predictions\n",
    "        result = forest.predict(test_data_features)\n",
    "        print(result)\n",
    "\n",
    "    # Press ctrl-c or ctrl-d on the keyboard to exit\n",
    "    except (KeyboardInterrupt, EOFError, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
